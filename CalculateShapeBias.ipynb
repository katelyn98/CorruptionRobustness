{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CalculateShapeBias.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p690e5H5nEBC",
        "al8EtXIMwLB5",
        "wRJDMgi6m42M",
        "j0yiQNOG-116",
        "65ULoZFUmVHn",
        "KUBdIl9y4V3d",
        "W2DCho3fSl4u",
        "vKQ98Nb8mbm1",
        "jsMHllR2mZ5b",
        "1cuk7KtG_vAo",
        "gNn_fTM6meYz",
        "gnM0R2fvmxv0",
        "571KOQvG_0YE",
        "uJIFE3wp-Nrb",
        "z4pyMIO0-7Zr"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katelyn98/CorruptionRobustness/blob/main/CalculateShapeBias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p690e5H5nEBC"
      },
      "source": [
        "## Install & import required libraries / repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fbupj-FKY5i"
      },
      "source": [
        "!pip3 install pytorch_pretrained_vit\n",
        "!pip3 install timm\n",
        "!pip3 install transformers\n",
        "\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import os\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from pytorch_pretrained_vit import ViT\n",
        "import timm\n",
        "from torch.utils import model_zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJONYai9MbI2"
      },
      "source": [
        "!git clone https://github.com/rgeirhos/texture-vs-shape.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkHLrhTrt15A"
      },
      "source": [
        "!mv texture-vs-shape/code/probabilities_to_decision.py ./ && mv texture-vs-shape/code/* ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3js9Y0ZUVuL"
      },
      "source": [
        "import probabilities_to_decision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnlMsaV1LWn6"
      },
      "source": [
        "STIMULI = \"texture-vs-shape/stimuli/style-transfer-preprocessed-512/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al8EtXIMwLB5"
      },
      "source": [
        "## Transformation Constant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQvbOVbywOlz"
      },
      "source": [
        "center_crop = 224\n",
        "preprocess = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(center_crop),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg4_nPeRnMOA"
      },
      "source": [
        "## Function to calculate shape bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkJyS6sYM3Le"
      },
      "source": [
        "#get the images collectively\n",
        "def calculate_shape_bias(dir, preprocess, model):\n",
        "  root_dir = dir\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  shape = 0\n",
        "  texture = 0\n",
        "  total = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for label in os.listdir(root_dir): #for every folder in the directory\n",
        "    data_dir = Path(root_dir) / label #go into that folder\n",
        "    data_files = data_dir.glob('*.png') #gather the images by .png name\n",
        "    \n",
        "    for image in data_files: #for every image in the folder\n",
        "        images.append(image) #add the image path to the list\n",
        "        labels.append(label) #add the folder name to the list of labels\n",
        "\n",
        "        shape_type = label \n",
        "\n",
        "        #get texture type from file name\n",
        "        types = str(image).split('/')\n",
        "        types = types[4]\n",
        "        typenum = types.split('-')\n",
        "        typenum = typenum[1]\n",
        "        texture_type = typenum.split('.')\n",
        "        texture_type = texture_type[0]\n",
        "        texture_type = texture_type[:-1]\n",
        "\n",
        "        input = Image.open(image)\n",
        "        input_tensor = preprocess(input)\n",
        "        input_batch = input_tensor.unsqueeze(0)\n",
        "        input_batch = input_batch.to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "          output_probs = model(input_batch)\n",
        "\n",
        "          ##############################################\n",
        "          ## Code from Robert Geirhos: https://github.com/rgeirhos/texture-vs-shape#code ##\n",
        "\n",
        "          softmax_output = torch.nn.functional.softmax(output_probs[0], dim=0)\n",
        "\n",
        "          # convert to numpy\n",
        "          softmax_output_numpy = softmax_output.cpu().numpy() # replace with conversion\n",
        "\n",
        "          # create mapping\n",
        "          mapping = probabilities_to_decision.ImageNetProbabilitiesTo16ClassesMapping()\n",
        "          \n",
        "          # obtain decision \n",
        "          decision_from_16_classes = mapping.probabilities_to_decision(softmax_output_numpy)\n",
        "          \n",
        "          ##############################################\n",
        "\n",
        "          if decision_from_16_classes == shape_type:\n",
        "            shape += 1\n",
        "          \n",
        "          if decision_from_16_classes == texture_type:\n",
        "            texture += 1\n",
        "\n",
        "          total += 1\n",
        "\n",
        "  print(\"SHAPE CORRECT TOTAL\")\n",
        "  print(shape)\n",
        "\n",
        "  print(\"TEXTURE CORRECT TOTAL\")\n",
        "  print(texture)\n",
        "\n",
        "  print(\"TOTAL IMAGES\")\n",
        "  print(total)\n",
        "\n",
        "  print(\"SHAPE BIAS\")\n",
        "  print(shape / (shape + texture))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGS2Do_mV7F0"
      },
      "source": [
        "## Function to calculate the number of parameters of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-msqww_wWAbq"
      },
      "source": [
        "def num_params(model):\n",
        "  return sum(p.numel() for p in model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd9wxV-OpZWl"
      },
      "source": [
        "## Choose model to calculate shape bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ORRRNCJlpea7"
      },
      "source": [
        "#@title Choose your model to evaluate\n",
        "\n",
        "model_choice = \"ViT\" #@param [\"ViT\", \"CaiT\", \"DeiT\", \"Swin-T\", \"MLP-Mixer\", \"ResNet50\", \"AlexNet\", \"VGG\", \"GoogLeNet\"]\n",
        "\n",
        "exact_model = \"ViT_B16\" #@param [\"ViT_B16\", \"ViT_L16\", \"DeiT_B16\", \"DeiT_B16_Distilled\", \"DeiT_S16\", \"DeiT_S16_Distilled\", \"DeiT_T16\", \"DeiT_T16_Distilled\", 'CaiT_S24', 'CaiT_XXS24', 'Swin-T_B', ''Swin-T_L', 'Swin-T_S', 'Swin-T_T', 'mixer_b16', 'mixer_l16']\n",
        "\n",
        "if exact_model == 'ViT_B16':\n",
        "  if model_choice != \"ViT\":\n",
        "    print(\"please set model_choice to 'ViT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "      print(\"Using model vit_base_patch16_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('vit_base_patch16_384', pretrained=True)\n",
        "      print(\"Using model vit_base_patch16_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'ViT_L16':\n",
        "  if model_choice != \"ViT\":\n",
        "    print(\"please set model_choice to 'ViT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_large_patch16_224', pretrained=True)\n",
        "      print(\"Using model vit_large_patch16_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('vit_large_patch16_384', pretrained=True)\n",
        "      print(\"Using model vit_large_patch16_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'DeiT_B16':\n",
        "  if model_choice != \"DeiT\":\n",
        "    print(\"please set model_choice to 'DeiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_deit_base_patch16_224', pretrained=True)\n",
        "      print(\"Using model vit_deit_base_patch16_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('vit_deit_base_patch16_384', pretrained=True)\n",
        "      print(\"Using model vit_deit_base_patch16_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "elif exact_model == \"DeiT_B16_Distilled\":\n",
        "  if model_choice != \"DeiT\":\n",
        "    print(\"please set model_choice to 'DeiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=True)\n",
        "      print(\"Using model vit_deit_base_distilled_patch16_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=True)\n",
        "      print(\"Using model vit_deit_base_distilled_patch16_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "elif exact_model == 'DeiT_S16':\n",
        "  if model_choice != \"DeiT\":\n",
        "    print(\"please set model_choice to 'DeiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_deit_small_patch16_224', pretrained = True)\n",
        "      print(\"Using model vit_deit_small_patch16_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "elif exact_model == 'DeiT_S16_Distilled':\n",
        "  if model_choice != \"DeiT\":\n",
        "    print(\"please set model_choice to 'DeiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained = True)\n",
        "      print(\"Using model vit_deit_small_distilled_patch16_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "elif exact_model == 'DeiT_T16':\n",
        "  if model_choice != \"DeiT\":\n",
        "    print(\"please set model_choice to 'DeiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_deit_tiny_patch16_224', pretrained=True)\n",
        "      print(\"Using model vit_deit_tiny_patch16_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "elif exact_model == 'DeiT_T16_Distilled':\n",
        "  if model_choice != \"DeiT\":\n",
        "    print(\"please set model_choice to 'DeiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=True)\n",
        "      print(\"Using model vit_deit_tiny_distilled_patch16_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'CaiT_S24':\n",
        "  if model_choice != \"CaiT\":\n",
        "    print(\"please set model_choice to 'CaiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('cait_s24_224', pretrained=True)\n",
        "      print(\"Using model cait_s24_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('cait_s24_384', pretrained=True)\n",
        "      print(\"Using model cait_s24_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'CaiT_XXS24':\n",
        "  if model_choice != \"CaiT\":\n",
        "    print(\"please set model_choice to 'CaiT'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('cait_xxs24_224', pretrained=True)\n",
        "      print(\"Using model cait_xxs24_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('cait_xxs24_384', pretrained=True)\n",
        "      print(\"Using model cait_xxs24_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'Swin-T_B':\n",
        "  if model_choice != \"Swin-T\":\n",
        "    print(\"please set model_choice to 'Swin-T'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
        "      print(\"Using model swin_base_patch4_window7_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('swin_base_patch4_window7_384', pretrained=True)\n",
        "      print(\"Using model swin_base_patch4_window7_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'Swin-T_L':\n",
        "  if model_choice != \"Swin-T\":\n",
        "    print(\"please set model_choice to 'Swin-T'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('swin_large_patch4_window7_224', pretrained=True)\n",
        "      print(\"Using model swin_large_patch4_window7_224\")\n",
        "    elif center_crop == 384:\n",
        "      model = timm.create_model('swin_large_patch4_window7_384', pretrained=True)\n",
        "      print(\"Using model swin_large_patch4_window7_384\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'Swin-T_S':\n",
        "  if model_choice != \"Swin-T\":\n",
        "    print(\"please set model_choice to 'Swin-T'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('swin_small_patch4_window7_224', pretrained=True)\n",
        "      print(\"Using model swin_small_patch4_window7_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'Swin-T_T':\n",
        "  if model_choice != \"Swin-T\":\n",
        "    print(\"please set model_choice to 'Swin-T'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True)\n",
        "      print(\"Using model swin_tiny_patch4_window7_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'mixer_b16':\n",
        "  if model_choice != \"MLP-Mixer\":\n",
        "    print(\"please set model_choice to 'MLP-Mixer'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('mixer_b16_224', pretrained=True)\n",
        "      print(\"Using model mixer_b16_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "elif exact_model == 'mixer_l16':\n",
        "  if model_choice != \"MLP-Mixer\":\n",
        "    print(\"please set model_choice to 'MLP-Mixer'\")\n",
        "  else:\n",
        "    if center_crop == 224:\n",
        "      model = timm.create_model('mixer_l16_224', pretrained=True)\n",
        "      print(\"Using model mixer_l16_224\")\n",
        "    else:\n",
        "      print(\"image size wrong\")\n",
        "\n",
        "model = model.to('cuda:0')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCOgQC0Epxvz"
      },
      "source": [
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkfS3_qxnBHz"
      },
      "source": [
        "# Vision Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRJDMgi6m42M"
      },
      "source": [
        "## Data-efficient Image Transformer ([DeiT](https://github.com/facebookresearch/deit?fbclid=IwAR2qzERDHwVdKSlah1v7MCsqp15EigeAjZbYp1F_YHm3ZR2-Bxkcejmq5r0))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEvZ4wP_yveU"
      },
      "source": [
        "# # pre-trained model from https://github.com/rwightman/pytorch-image-models\n",
        "# model = timm.create_model('vit_deit_base_patch16_224', pretrained=True)\n",
        "# model = model.to('cuda')\n",
        "\n",
        "# #calculate the number of parameters\n",
        "# print(\"NUM PARAMS\")\n",
        "# print(num_params(model)\n",
        "\n",
        "# calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgFvih8yWTfu"
      },
      "source": [
        "#deit model\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bn7jTU2y4f8"
      },
      "source": [
        "#deit model\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "augYYmobztu8"
      },
      "source": [
        "#deit model\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_distilled_patch16_224', pretrained=True)\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TJkFZb7z4v8"
      },
      "source": [
        "#deit model\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_small_patch16_224', pretrained=True)\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX6GeyEe-Y9r"
      },
      "source": [
        "#deit model\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_small_distilled_patch16_224', pretrained=True)\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ExVnavq_V33"
      },
      "source": [
        "#deit model\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_distilled_patch16_224', pretrained=True) # base_distilled\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0yiQNOG-116"
      },
      "source": [
        "## Class-Attention in Image Transformers ([CaiT](https://github.com/facebookresearch/deit/blob/main/README_cait.md))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YqJDjNp-4UV"
      },
      "source": [
        "#cait model\n",
        "model = timm.create_model('cait_s24_224', pretrained=True)\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNWeiSdWTpnE"
      },
      "source": [
        "#cait model\n",
        "model = timm.create_model('cait_xxs24_224', pretrained=True)\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ULoZFUmVHn"
      },
      "source": [
        "## Vision Transformer ([ViT](https://github.com/google-research/vision_transformer))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SriVe6uPlbO"
      },
      "source": [
        "# pre-trained model from https://github.com/rwightman/pytorch-image-models\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5VRleizLyIf"
      },
      "source": [
        "# pre-trained model from https://github.com/rwightman/pytorch-image-models\n",
        "model = timm.create_model('vit_large_patch16_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emq-akNHzrFA"
      },
      "source": [
        "# pre-trained model from https://github.com/rwightman/pytorch-image-models\n",
        "model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUBdIl9y4V3d"
      },
      "source": [
        "## Swin Transformer ([Swin-T](https://arxiv.org/abs/2103.14030)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y13jvKTO4Z8e"
      },
      "source": [
        "#pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLjkrq4D5fW7"
      },
      "source": [
        "# #pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "# model = timm.create_model('swin_base_patch4_window12_384', pretrained=True)\n",
        "# model = model.to('cuda')\n",
        "\n",
        "# #calculate the number of parameters\n",
        "# print(\"NUM PARAMS\")\n",
        "# print(num_params(model))\n",
        "\n",
        "# preprocess = transforms.Compose([\n",
        "#       transforms.Resize(384),\n",
        "#       transforms.CenterCrop(384),\n",
        "#       transforms.ToTensor(),\n",
        "#       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "#   ])\n",
        "\n",
        "# calculate_shape_bias(STIMULI, preprocess, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOjhC0tN5X0Z"
      },
      "source": [
        "#pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "model = timm.create_model('swin_large_patch4_window7_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMrkf5M46AL-"
      },
      "source": [
        "#pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "model = timm.create_model('swin_small_patch4_window7_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHmR5hhv6Iqi"
      },
      "source": [
        "#pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2DCho3fSl4u"
      },
      "source": [
        "# MLP-mixer ([Dosovitskiy, et. al., 2021](https://arxiv.org/abs/2105.01601))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6NPrpGwJoK5"
      },
      "source": [
        "!pip3 install timm\n",
        "import timm\n",
        "from pprint import pprint\n",
        "model_names = timm.list_models(pretrained=True)\n",
        "pprint(model_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKhVAoUKSnzi"
      },
      "source": [
        "#MLP-mixer pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "model = timm.create_model('mixer_b16_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9ZDFx0e4JXC"
      },
      "source": [
        "#MLP-mixer pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "model = timm.create_model('mixer_l16_224', pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKQ98Nb8mbm1"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsMHllR2mZ5b"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNwIxAOFUZ4c"
      },
      "source": [
        "#ResNet50\n",
        "model = models.resnet50(pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff_aX0Me66UL"
      },
      "source": [
        "# #pre-trained model from # https://github.com/rwightman/pytorch-image-models\n",
        "\n",
        "# model = timm.create_model('resnet50', pretrained=True)\n",
        "# model = model.to('cuda')\n",
        "\n",
        "# #calculate the number of parameters\n",
        "# print(\"NUM PARAMS\")\n",
        "# print(num_params(model))\n",
        "\n",
        "# calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cuk7KtG_vAo"
      },
      "source": [
        "## AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIagV2C__uIj"
      },
      "source": [
        "#calculate shape bias of AlexNet\n",
        "model = models.alexnet(pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNn_fTM6meYz"
      },
      "source": [
        "## ResNet50 trained on Stylized-ImageNet ([Geirhos, et. al., 2019](https://github.com/rgeirhos/texture-vs-shape))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSRRnkmZU66g"
      },
      "source": [
        "#SIN\n",
        "model = torchvision.models.resnet50(pretrained=False)\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "checkpoint = model_zoo.load_url('https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/6f41d2e86fc60566f78de64ecff35cc61eb6436f/resnet50_train_60_epochs-c8e5653e.pth.tar')\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnM0R2fvmxv0"
      },
      "source": [
        "## ResNet50 trained on Stylized-ImageNet and ImageNet ([Geirhos, et. al., 2019](https://github.com/rgeirhos/texture-vs-shape))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rmYggwMZpii"
      },
      "source": [
        "#SIN + IN\n",
        "model = torchvision.models.resnet50(pretrained=False)\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "checkpoint = model_zoo.load_url('https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/60b770e128fffcbd8562a3ab3546c1a735432d03/resnet50_train_45_epochs_combined_IN_SF-2a0d100e.pth.tar')\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "571KOQvG_0YE"
      },
      "source": [
        "## AlexNet trained on Stylized ImageNet ([Geirhos, et. al., 2019](https://github.com/rgeirhos/texture-vs-shape))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZgaLqz9_sIh"
      },
      "source": [
        "# AlexNet on SIN\n",
        "model = torchvision.models.alexnet(pretrained=False)\n",
        "model.features = torch.nn.DataParallel(model.features)\n",
        "model.cuda()\n",
        "checkpoint = model_zoo.load_url('https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/0008049cd10f74a944c6d5e90d4639927f8620ae/alexnet_train_60_epochs_lr0.001-b4aa5238.pth.tar')\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJIFE3wp-Nrb"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmxC-saA-Qf5"
      },
      "source": [
        "#VGG\n",
        "model = models.vgg16(pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4pyMIO0-7Zr"
      },
      "source": [
        "## GoogLeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdubgivf_T1m"
      },
      "source": [
        "#GoogLeNet\n",
        "model = models.googlenet(pretrained=True)\n",
        "model = model.to('cuda')\n",
        "\n",
        "#calculate the number of parameters\n",
        "print(\"NUM PARAMS\")\n",
        "print(num_params(model))\n",
        "\n",
        "calculate_shape_bias(STIMULI, preprocess, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcmIjnsAZ3zO",
        "outputId": "4aee99fe-606c-4a46-f389-5634008d2c12"
      },
      "source": [
        "!ls -l /content/texture-vs-shape/stimuli/style-transfer-preprocessed-512/car | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}